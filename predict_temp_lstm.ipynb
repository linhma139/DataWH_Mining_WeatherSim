{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4055f25f-7d4d-4a1a-827e-396ad74235a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read and filter fields"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "import pandas as pd\n",
    "\n",
    "df = spark.read.table(\"hcmut.gold.fact_vn_weather_hourly\")\n",
    "selected_columns = [\n",
    "    \"dt_date_record\",\n",
    "    \"ds_location\",\n",
    "    \"nr_temperature_2m\",\n",
    "    \"nr_dew_point_2m\",\n",
    "    \"nr_relative_humidity_2m\",\n",
    "    \"nr_pressure_msl\",\n",
    "    \"nr_precipitation\",\n",
    "    \"nr_cloud_cover\",\n",
    "    \"nr_wind_speed_10m\",\n",
    "    \"nr_wind_direction_10m\",\n",
    "    \"nr_sunshine_duration\"\n",
    "]\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"loc_hcm\",\n",
    "    when(col(\"ds_location\") == \"Ho Chi Minh City\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"loc_dn\",\n",
    "    when(col(\"ds_location\") == \"Da Nang City\", 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"loc_hn\",\n",
    "    when(col(\"ds_location\") == \"Ha Noi City\", 1).otherwise(0)\n",
    ")\n",
    "selected_columns.extend([\"loc_hcm\", \"loc_dn\", \"loc_hn\"])\n",
    "\n",
    "df = (\n",
    "    df.select(*selected_columns)\n",
    "      .dropna()\n",
    "      .dropDuplicates([\"dt_date_record\"])\n",
    "      .orderBy(\"dt_date_record\")\n",
    ")\n",
    "\n",
    "pdf = df.toPandas()\n",
    "pdf['dt_date_record'] = pd.to_datetime(pdf['dt_date_record'])\n",
    "pdf = pdf.set_index('dt_date_record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480816e5-8643-47cf-8d73-92c1d9c2692b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalization data"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_cols = [\n",
    "    \"nr_temperature_2m\",\n",
    "    \"nr_dew_point_2m\",\n",
    "    \"nr_relative_humidity_2m\",\n",
    "    \"nr_pressure_msl\",\n",
    "    \"nr_precipitation\",\n",
    "    \"nr_cloud_cover\",\n",
    "    \"nr_wind_speed_10m\",\n",
    "    \"nr_wind_direction_10m\",\n",
    "    \"nr_sunshine_duration\"\n",
    "]\n",
    "\n",
    "location_cols = [\"loc_hcm\", \"loc_dn\", \"loc_hn\"]\n",
    "scaler = MinMaxScaler()\n",
    "scaled_numeric = scaler.fit_transform(pdf[numeric_cols])\n",
    "\n",
    "scaled_data = np.hstack([scaled_numeric, pdf[location_cols].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e06f9b-a86b-493e-915f-790512a066aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create series value and next value"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. ĐỊNH NGHĨA CÁC THAM SỐ CẤU HÌNH ---\n",
    "# (Hãy đảm bảo các biến này khớp với mô hình của bạn)\n",
    "\n",
    "# scaled_data = ... (mảng numpy (total_rows, 12) của bạn từ cell trước)\n",
    "# pdf = ...           (Pandas DataFrame từ cell trước)\n",
    "\n",
    "N_INPUT_HOURS = 48  # (Số giờ input, ví dụ: 48)\n",
    "N_OUTPUT_HOURS = 24 # (Số giờ output, PHẢI là 24)\n",
    "TARGET_COL_INDEX = 0 # (Vị trí cột 'nr_temperature_2m', là 0)\n",
    "\n",
    "# --- 2. TÁCH TRAIN/TEST (THEO THỜI GIAN) ---\n",
    "# Chúng ta phải tách trước khi tạo chuỗi để tránh rò rỉ dữ liệu.\n",
    "# KHÔNG BAO GIỜ dùng train_test_split(..., shuffle=True) cho time series.\n",
    "\n",
    "test_split_percentage = 0.2\n",
    "test_split_index = int(len(scaled_data) * (1 - test_split_percentage))\n",
    "\n",
    "train_data = scaled_data[:test_split_index]\n",
    "test_data = scaled_data[test_split_index:]\n",
    "\n",
    "# (Tùy chọn) Lưu lại index thời gian để vẽ biểu đồ\n",
    "train_index = pdf.index[:test_split_index]\n",
    "test_index = pdf.index[test_split_index:]\n",
    "\n",
    "print(f\"--- TÁCH DỮ LIỆU ---\")\n",
    "print(f\"Total data shape: {scaled_data.shape}\")\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# --- 3. ĐỊNH NGHĨA HÀM TẠO CHUỖI (ĐÃ SỬA) ---\n",
    "# Hàm này sẽ tạo y_train có shape (samples, 24)\n",
    "\n",
    "def create_sequences(data, n_input, n_output, target_col_index):\n",
    "    X_list = [] # Danh sách chứa các chuỗi input\n",
    "    y_list = [] # Danh sách chứa các chuỗi output (24 giờ)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Tìm điểm cuối của 2 cửa sổ (input và output)\n",
    "        end_x = i + n_input\n",
    "        end_y = end_x + n_output\n",
    "        \n",
    "        # Kiểm tra xem có vượt quá giới hạn mảng không\n",
    "        if end_y > len(data):\n",
    "            break\n",
    "            \n",
    "        # Lấy X (input): shape (n_input, n_features)\n",
    "        # Tức là (48, 12)\n",
    "        seq_x = data[i:end_x, :]\n",
    "        X_list.append(seq_x)\n",
    "        \n",
    "        # Lấy y (target): shape (n_output,)\n",
    "        # Tức là (24,)\n",
    "        # CHỈ lấy cột mục tiêu (nhiệt độ)\n",
    "        seq_y = data[end_x:end_y, target_col_index]\n",
    "        y_list.append(seq_y)\n",
    "        \n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# --- 4. TẠO DỮ LIỆU HUẤN LUYỆN VÀ KIỂM THỬ ---\n",
    "print(\"\\nĐang tạo chuỗi cho X_train, y_train...\")\n",
    "X_train, y_train = create_sequences(train_data, N_INPUT_HOURS, N_OUTPUT_HOURS, TARGET_COL_INDEX)\n",
    "\n",
    "print(\"Đang tạo chuỗi cho X_test, y_test...\")\n",
    "X_test, y_test = create_sequences(test_data, N_INPUT_HOURS, N_OUTPUT_HOURS, TARGET_COL_INDEX)\n",
    "\n",
    "# (Tùy chọn) Lưu lại index cho y_test (dùng để vẽ biểu đồ)\n",
    "# Cần logic phức tạp hơn một chút để lấy đúng index cho y_test\n",
    "# (Bỏ qua nếu bạn không cần vẽ biểu đồ)\n",
    "\n",
    "\n",
    "# --- 5. (TRẢ LỜI) KIỂM TRA SHAPE ---\n",
    "print(\"\\n--- KIỂM TRA SHAPE (QUAN TRỌNG) ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n--- KIỂM TRA 1 MẪU ---\")\n",
    "print(f\"Một mẫu X_train (shape {X_train[0].shape}): \\n{X_train[0][:2]}...\") # In 2 dòng đầu của mẫu đầu tiên\n",
    "print(f\"Một mẫu y_train (shape {y_train[0].shape}): \\n{y_train[0]}...\") # In 24 giá trị của mẫu đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0af3fdec-9995-4b98-b194-00bb81a800ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train model"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(32),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(24)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.1, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81baf3cf-af97-4a15-9529-38f83feda3cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Evaluation model"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "n_numeric = len(numeric_cols) # (Đây là 9)\n",
    "\n",
    "# 1. Lấy dự đoán từ model\n",
    "# Shape của X_test là (10229, 48, 12)\n",
    "# Shape của y_pred_scaled sẽ là (10229, 24)\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# 2. Làm dẹt (flatten) CẢ HAI mảng dự đoán và mảng thực tế\n",
    "# Shape của cả hai sẽ trở thành (10229 * 24,) = (245496,)\n",
    "y_pred_flat = y_pred_scaled.reshape(-1)\n",
    "y_test_flat = y_test.reshape(-1) # <--- y_test của bạn đã có shape (10229, 24)\n",
    "\n",
    "# 3. Tạo các \"dummy array\" LỚN tương ứng\n",
    "# Chúng phải có shape (245496, 9)\n",
    "dummy_pred = np.zeros((len(y_pred_flat), n_numeric))\n",
    "dummy_real = np.zeros((len(y_test_flat), n_numeric))\n",
    "\n",
    "# 4. Đặt dữ liệu đã làm dẹt vào cột 0 (cột nhiệt độ)\n",
    "dummy_pred[:, 0] = y_pred_flat\n",
    "dummy_real[:, 0] = y_test_flat\n",
    "\n",
    "# 5. Bây giờ mới Inverse Transform\n",
    "# Cả hai mảng đầu ra sẽ có shape (245496,)\n",
    "y_pred_unscaled = scaler.inverse_transform(dummy_pred)[:, 0]\n",
    "y_test_unscaled = scaler.inverse_transform(dummy_real)[:, 0]\n",
    "\n",
    "# 6. Tính toán Metrics trên toàn bộ dữ liệu\n",
    "print(\"--- Đánh giá trên toàn bộ 24 giờ dự đoán ---\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_unscaled, y_pred_unscaled))\n",
    "r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "mae = mean_absolute_error(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "# Xử lý trường hợp chia cho 0 (mặc dù hiếm với nhiệt độ)\n",
    "mask = y_test_unscaled != 0\n",
    "mape = np.mean(np.abs((y_test_unscaled[mask] - y_pred_unscaled[mask]) / y_test_unscaled[mask])) * 100\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f} °C\")\n",
    "print(f\"R²  : {r2:.2f}\")\n",
    "print(f\"MAE : {mae:.3f} °C\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c2af09-5172-4620-beee-7c25821b210f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualize Predict vs Real data"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "location_name = \"Ho Chi Minh City\"\n",
    "location_map = {\n",
    "    \"Ho Chi Minh City\": [1,0,0],\n",
    "    \"Da Nang City\": [0,1,0],\n",
    "    \"Ha Noi City\": [0,0,1],\n",
    "}\n",
    "location_onehot = np.array(location_map[location_name])\n",
    "\n",
    "pdf_loc = pdf[pdf['ds_location'] == location_name].copy()\n",
    "if len(pdf_loc) < 48:\n",
    "    raise ValueError(f\"Not enough data for {location_name} to forecast 48h\")\n",
    "\n",
    "last_48_indexed = pdf_loc.index[-48:]\n",
    "last_48 = scaled_data[-48:].copy() \n",
    "last_48[:, -3:] = location_onehot\n",
    "\n",
    "def forecast_next_24_hours(model, last_seq, location_onehot, n_steps=24):\n",
    "    seq = last_seq.copy()\n",
    "    predictions = []\n",
    "    for _ in range(n_steps):\n",
    "        X = seq.reshape(1, seq.shape[0], seq.shape[1])\n",
    "        pred_scaled = model.predict(X)[0][0]\n",
    "        predictions.append(pred_scaled)\n",
    "\n",
    "        fake_record = seq[-1].copy()\n",
    "        fake_record[0] = pred_scaled\n",
    "        fake_record[-3:] = location_onehot\n",
    "        seq = np.vstack([seq[1:], fake_record])\n",
    "    return np.array(predictions)\n",
    "\n",
    "pred_scaled_24 = forecast_next_24_hours(model, last_48, location_onehot)\n",
    "dummy_forecast = np.zeros((len(pred_scaled_24), len(numeric_cols)))\n",
    "dummy_forecast[:, 0] = pred_scaled_24\n",
    "pred_24_real = scaler.inverse_transform(dummy_forecast)[:, 0]\n",
    "\n",
    "last_48_index_test = pdf_loc.index[-48:]\n",
    "last_48_real = y_test_real[-48:]\n",
    "last_48_pred = y_pred[-48:]\n",
    "\n",
    "vn_now = last_48_index_test[-1]\n",
    "forecast_index = [vn_now + pd.Timedelta(hours=i+1) for i in range(24)]\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(last_48_index_test, last_48_real, marker='o', label=\"Real (Last 48h Test)\")\n",
    "plt.plot(last_48_index_test, last_48_pred, marker='x', label=\"Predicted (Last 48h Test)\")\n",
    "\n",
    "plt.plot(forecast_index, pred_24_real, marker='s', linestyle='--', label=\"Forecast Next 24h\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(byhour=range(0,24,6))) \n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m %H:%M'))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.title(f\"48h Test and 24h Forecast ({location_name})\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0213a225-686d-49ec-9cc0-35acb35f4994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- BƯỚC 7 (GỘP VÀ CLEAN): LOGGING VỚI UNITY CATALOG & SIGNATURE ---\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 7.1. Lấy các biến đã huấn luyện từ các cell trước ---\n",
    "# (Đảm bảo các biến này đã tồn tại sau khi huấn luyện)\n",
    "# N_INPUT_HOURS, N_OUTPUT_HOURS, numeric_cols, location_cols, \n",
    "# FEATURE_COLUMNS, TARGET_COL_INDEX, model, scaler, pdf, history\n",
    "\n",
    "try:\n",
    "    # Gán vào tên biến chuẩn\n",
    "    trained_model = model\n",
    "    trained_scaler = scaler\n",
    "    \n",
    "    print(\"Đã xác định các biến huấn luyện (model, scaler, pdf, history, etc.).\")\n",
    "    print(f\"Input hours: {N_INPUT_HOURS}, Output hours: {N_OUTPUT_HOURS}\")\n",
    "    print(f\"Target column index: {TARGET_COL_INDEX}\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"LỖI: Không tìm thấy biến cần thiết. Bạn đã chạy cell huấn luyện chưa?\")\n",
    "    print(f\"Chi tiết: {e}\")\n",
    "    # dbutils.notebook.exit(\"Biến huấn luyện không tồn tại\")\n",
    "\n",
    "\n",
    "# --- 7.2. Định nghĩa Lớp Wrapper (xử lý \"scale một phần\") ---\n",
    "\n",
    "class LSTMWeatherWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self, model, scaler, n_input_hours, n_output_hours, \n",
    "                 numeric_cols, location_cols, target_col_index):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.n_input = n_input_hours\n",
    "        self.n_output = n_output_hours\n",
    "        \n",
    "        # Lưu lại danh sách các cột\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.location_cols = location_cols\n",
    "        self.feature_columns = numeric_cols + location_cols\n",
    "        \n",
    "        self.n_features = len(self.feature_columns)\n",
    "        self.n_numeric_features = len(numeric_cols)\n",
    "        self.target_idx = target_col_index\n",
    "\n",
    "    def _preprocess(self, model_input_df):\n",
    "        # Đảm bảo DataFrame có đúng các cột\n",
    "        processed_df = model_input_df[self.feature_columns]\n",
    "        \n",
    "        # 1. Tách riêng 2 nhóm cột\n",
    "        numeric_data = processed_df[self.numeric_cols]\n",
    "        location_data = processed_df[self.location_cols].values\n",
    "        \n",
    "        # 2. CHỈ scale các cột numeric\n",
    "        scaled_numeric = self.scaler.transform(numeric_data)\n",
    "        \n",
    "        # 3. Kết hợp lại (giống hệt code training)\n",
    "        scaled_data = np.hstack([scaled_numeric, location_data])\n",
    "        \n",
    "        # 4. Reshape cho LSTM [samples, timesteps, features]\n",
    "        input_sequence = scaled_data.reshape(1, self.n_input, self.n_features)\n",
    "        return input_sequence\n",
    "\n",
    "    def _postprocess(self, prediction_scaled):\n",
    "        # prediction_scaled có shape (1, n_output)\n",
    "        \n",
    "        # Tạo một mảng \"giả\" (dummy) CHỈ có shape (n_output, n_numeric_features)\n",
    "        dummy_array = np.zeros((self.n_output, self.n_numeric_features))\n",
    "        \n",
    "        # Đặt kết quả dự đoán (đã chuẩn hóa) vào đúng cột mục tiêu\n",
    "        dummy_array[:, self.target_idx] = prediction_scaled\n",
    "        \n",
    "        # 3. Inverse transform (Biến đổi ngược)\n",
    "        prediction_unscaled = self.scaler.inverse_transform(dummy_array)\n",
    "        \n",
    "        # 4. Chỉ lấy cột mục tiêu (nhiệt độ) đã được unscale\n",
    "        final_forecast = prediction_unscaled[:, self.target_idx]\n",
    "        \n",
    "        return final_forecast\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # 'model_input' là một Pandas DataFrame của (N_INPUT_HOURS, N_FEATURES)\n",
    "        \n",
    "        if model_input.shape[0] != self.n_input:\n",
    "            raise ValueError(f\"Input data must have exactly {self.n_input} rows (timesteps).\")\n",
    "            \n",
    "        # Bước 1-4: Tiền xử lý (Tách, Scale, Hstack, Reshape)\n",
    "        input_sequence = self._preprocess(model_input)\n",
    "        \n",
    "        # Bước 5: Dự đoán\n",
    "        prediction_scaled = self.model.predict(input_sequence)\n",
    "        \n",
    "        # Bước 6-7: Hậu xử lý (Inverse Transform)\n",
    "        final_forecast = self._postprocess(prediction_scaled.flatten())\n",
    "        \n",
    "        return final_forecast.tolist() # Trả về list 24 con số\n",
    "\n",
    "print(\"Đã định nghĩa lớp 'LSTMWeatherWrapper'.\")\n",
    "\n",
    "# --- 7.3. Tạo Signature và Logging vào Unity Catalog ---\n",
    "\n",
    "# 1. TẠO INPUT VÀ OUTPUT MẪU ĐỂ SUY RA SIGNATURE\n",
    "print(\"Đang tạo đối tượng wrapper...\")\n",
    "pyfunc_wrapper = LSTMWeatherWrapper(\n",
    "    model=trained_model,\n",
    "    scaler=trained_scaler,\n",
    "    n_input_hours=N_INPUT_HOURS,\n",
    "    n_output_hours=N_OUTPUT_HOURS,\n",
    "    numeric_cols=numeric_cols,\n",
    "    location_cols=location_cols,\n",
    "    target_col_index=TARGET_COL_INDEX\n",
    ")\n",
    "\n",
    "print(\"Đang chuẩn bị dữ liệu mẫu cho signature...\")\n",
    "try:\n",
    "    # Lấy một mẫu input (ví dụ: N_INPUT_HOURS giờ đầu tiên từ 'pdf')\n",
    "    input_example = pdf[FEATURE_COLUMNS].iloc[0:N_INPUT_HOURS]\n",
    "    \n",
    "    if input_example.shape[0] != N_INPUT_HOURS:\n",
    "        raise ValueError(f\"Dữ liệu 'pdf' không đủ {N_INPUT_HOURS} dòng để làm mẫu.\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"LỖI: Không tìm thấy 'pdf'. Bạn phải chạy lại cell training để có 'pdf'.\")\n",
    "    # dbutils.notebook.exit(\"Cần 'pdf' để tạo signature\")\n",
    "except Exception as e:\n",
    "    print(f\"LỖI khi lấy input_example: {e}\")\n",
    "    # dbutils.notebook.exit(\"Lỗi signature\")\n",
    "\n",
    "print(f\"Đang chạy dự đoán mẫu trên {input_example.shape[0]} dòng...\")\n",
    "# Chạy dự đoán mẫu để lấy output\n",
    "output_example = pyfunc_wrapper.predict(context=None, model_input=input_example)\n",
    "\n",
    "# 2. SUY RA SIGNATURE\n",
    "print(\"Đang suy ra signature từ input và output mẫu...\")\n",
    "signature = infer_signature(input_example, output_example)\n",
    "print(\"--- Đã tạo signature thành công ---\")\n",
    "\n",
    "\n",
    "# 3. LOG VÀ ĐĂNG KÝ MÔ HÌNH (VỚI SIGNATURE)\n",
    "\n",
    "# !!! QUAN TRỌNG: Sửa lại <CATALOG_NAME> và <SCHEMA_NAME> của bạn\n",
    "UC_CATALOG_NAME = \"hcmut\"  # !!! THAY THẾ: Bằng Catalog của bạn\n",
    "UC_SCHEMA_NAME = \"gold\"    # !!! THAY THẾ: Bằng Schema của bạn (ví dụ: 'gold' hoặc 'ml')\n",
    "MODEL_NAME = \"WeatherForecast_LSTM_MultiCity\"\n",
    "model_registry_name = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "\n",
    "# Set Model Registry về UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Đặt experiment\n",
    "experiment_name = f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/LSTM_Weather_Forecast\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Bắt đầu logging vào Experiment: {experiment_name}\")\n",
    "print(f\"Đăng ký mô hình UC với tên: {model_registry_name}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LSTM_24h_Forecast_UC_Run\") as run:\n",
    "    \n",
    "    # Log các tham số\n",
    "    mlflow.log_param(\"n_input_hours\", N_INPUT_HOURS)\n",
    "    mlflow.log_param(\"n_output_hours\", N_OUTPUT_HOURS)\n",
    "    mlflow.log_param(\"feature_columns_count\", len(FEATURE_COLUMNS))\n",
    "\n",
    "    # (Tùy chọn) Log các chỉ số\n",
    "    if 'history' in locals():\n",
    "        mlflow.log_metric(\"final_val_loss\", history.history['val_loss'][-1])\n",
    "        mlflow.log_metric(\"final_train_loss\", history.history['loss'][-1])\n",
    "\n",
    "    # 4. LOG MÔ HÌNH (Sử dụng đối tượng wrapper đã tạo ở trên)\n",
    "    print(\"Đang log mô hình với signature...\")\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=pyfunc_wrapper,\n",
    "        registered_model_name=model_registry_name,\n",
    "        signature=signature  # <-- THÊM SIGNATURE VÀO ĐÂY\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"--- HOÀN TẤT LOGGING (với Signature) ---\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Đã đăng ký phiên bản mới cho mô hình: '{model_registry_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6635920-9c2d-4b31-9a77-f85bf6794f6e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763216346341}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- BƯỚC 8 (ĐÃ SỬA): PIPELINE DỰ BÁO TỰ ĐỘNG ---\n",
    "# (Tương thích với Unity Catalog Aliases)\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, when\n",
    "import datetime\n",
    "\n",
    "# --- 8.1. Các tham số cấu hình ---\n",
    "\n",
    "# !!! QUAN TRỌNG: Sửa lại <CATALOG_NAME> và <SCHEMA_NAME> của bạn\n",
    "UC_CATALOG_NAME = \"hcmut\"\n",
    "UC_SCHEMA_NAME = \"gold\"\n",
    "MODEL_NAME = \"WeatherForecast_LSTM_MultiCity\"\n",
    "MODEL_REGISTRY_NAME = f\"{UC_CATALOG_NAME}.{UC_SCHEMA_NAME}.{MODEL_NAME}\"\n",
    "\n",
    "# --- (SỬA 1) ---\n",
    "# MODEL_STAGE = \"Production\" # <-- BỎ DÒNG NÀY\n",
    "MODEL_ALIAS = \"prod\" # <-- THÊM DÒNG NÀY (Phải khớp với alias bạn đặt ở Bước 1)\n",
    "# ---------------\n",
    "\n",
    "# Tên bảng Delta Lake (nguồn)\n",
    "SOURCE_TABLE = \"hcmut.gold.fact_vn_weather_hourly\"\n",
    "# Tên bảng Delta Lake (đích) để lưu kết quả\n",
    "TARGET_TABLE = \"hcmut.gold.lstm_weather_24h\"\n",
    "\n",
    "# Các thành phố cần dự báo\n",
    "CITIES_TO_FORECAST = [\"Ho Chi Minh City\", \"Da Nang City\", \"Ha Noi City\"]\n",
    "\n",
    "# Các thông số của mô hình (PHẢI GIỐNG HỆT KHI HUẤN LUYỆN)\n",
    "N_INPUT_HOURS = 48 \n",
    "N_OUTPUT_HOURS = 24\n",
    "\n",
    "numeric_cols = [\n",
    "    \"nr_temperature_2m\", \"nr_dew_point_2m\", \"nr_relative_humidity_2m\",\n",
    "    \"nr_pressure_msl\", \"nr_precipitation\", \"nr_cloud_cover\",\n",
    "    \"nr_wind_speed_10m\", \"nr_wind_direction_10m\", \"nr_sunshine_duration\"\n",
    "]\n",
    "location_cols = [\"loc_hcm\", \"loc_dn\", \"loc_hn\"]\n",
    "FEATURE_COLUMNS = numeric_cols + location_cols\n",
    "BASE_COLUMNS_TO_SELECT = [\"dt_date_record\", \"ds_location\"] + numeric_cols\n",
    "\n",
    "# --- 8.2. Tải Mô hình \"All-in-One\" từ Registry ---\n",
    "\n",
    "# Set registry về Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# --- (SỬA 2) ---\n",
    "print(f\"Đang tải mô hình '{MODEL_REGISTRY_NAME}' với bí danh (alias) '@{MODEL_ALIAS}'...\")\n",
    "# Cú pháp mới: models:/<model_name>@<alias>\n",
    "model_uri = f\"models:/{MODEL_REGISTRY_NAME}@{MODEL_ALIAS}\"\n",
    "# ---------------\n",
    "\n",
    "try:\n",
    "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "    print(\"--- Tải mô hình thành công ---\")\n",
    "except Exception as e:\n",
    "    # --- (SỬA 3) ---\n",
    "    print(f\"LỖI: Không thể tải mô hình. Bạn đã đặt alias '@{MODEL_ALIAS}' cho phiên bản mới nhất chưa?\")\n",
    "    print(f\"Chi tiết lỗi: {e}\")\n",
    "    dbutils.notebook.exit(f\"Model loading failed. Check alias '@{MODEL_ALIAS}'.\") # <-- KÍCH HOẠT DÒNG NÀY\n",
    "    # ---------------\n",
    "\n",
    "# --- 8.3. Lấy Dữ liệu, Xử lý và Dự báo (Lặp qua từng thành phố) ---\n",
    "\n",
    "all_forecasts = [] \n",
    "\n",
    "for city_name in CITIES_TO_FORECAST:\n",
    "    print(f\"\\n--- Đang xử lý cho: {city_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Lấy N_INPUT_HOURS (ví dụ: 48) bản ghi mới nhất\n",
    "        print(f\"Đang lấy {N_INPUT_HOURS} giờ dữ liệu mới nhất...\")\n",
    "        latest_data_df = (\n",
    "            spark.read.table(SOURCE_TABLE)\n",
    "            .select(*BASE_COLUMNS_TO_SELECT)\n",
    "            .withColumn(\"loc_hcm\", when(col(\"ds_location\") == \"Ho Chi Minh City\", 1).otherwise(0))\n",
    "            .withColumn(\"loc_dn\", when(col(\"ds_location\") == \"Da Nang City\", 1).otherwise(0))\n",
    "            .withColumn(\"loc_hn\", when(col(\"ds_location\") == \"Ha Noi City\", 1).otherwise(0))\n",
    "            .filter(col(\"ds_location\") == city_name)\n",
    "            .select([\"dt_date_record\"] + FEATURE_COLUMNS) \n",
    "            .orderBy(col(\"dt_date_record\").desc())\n",
    "            .limit(N_INPUT_HOURS) \n",
    "            .orderBy(col(\"dt_date_record\").asc()) \n",
    "        ).toPandas() \n",
    "\n",
    "        if latest_data_df.shape[0] != N_INPUT_HOURS:\n",
    "            print(f\"LỖI: Không tìm thấy đủ {N_INPUT_HOURS} giờ dữ liệu cho {city_name}. Tìm thấy {latest_data_df.shape[0]} giờ. Bỏ qua...\")\n",
    "            continue \n",
    "\n",
    "        print(f\"Dữ liệu đầu vào (từ {latest_data_df['dt_date_record'].min()} đến {latest_data_df['dt_date_record'].max()})\")\n",
    "\n",
    "        # --- 8.4. Thực hiện Dự báo ---\n",
    "        print(\"Đang thực hiện dự báo...\")\n",
    "    \n",
    "        forecast_values = loaded_model.predict(latest_data_df[FEATURE_COLUMNS])\n",
    "\n",
    "        print(\"--- Dự báo thành công ---\")\n",
    "\n",
    "        # --- 8.5. Xử lý Kết quả (Cho thành phố này) ---\n",
    "        last_known_time = latest_data_df['dt_date_record'].max()\n",
    "        forecast_index = [\n",
    "            last_known_time + pd.Timedelta(hours=i + 1) for i in range(N_OUTPUT_HOURS)\n",
    "        ]\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'dt_forecast_time': forecast_index,\n",
    "            'nr_predicted_temperature': forecast_values,\n",
    "            'ds_location': city_name,\n",
    "            'dt_model_run_time': datetime.datetime.now(datetime.timezone.utc),\n",
    "            'ds_model_version_uri': model_uri # Lưu lại URI của mô hình đã dự báo\n",
    "        })\n",
    "        \n",
    "        all_forecasts.append(results_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"LỖI trong quá trình xử lý cho {city_name}: {e}\")\n",
    "        # (Nếu có lỗi ở đây, chúng ta vẫn tiếp tục với thành phố tiếp theo)\n",
    "\n",
    "# --- 8.6. Lưu Tất cả Kết quả vào Delta Lake ---\n",
    "\n",
    "if not all_forecasts:\n",
    "    print(\"\\nKhông có kết quả dự báo nào được tạo ra. Kết thúc.\")\n",
    "else:\n",
    "    print(f\"\\n--- TỔNG HỢP KẾT QUẢ DỰ BÁO ({len(all_forecasts)} thành phố) ---\")\n",
    "    final_results_df = pd.concat(all_forecasts)\n",
    "    display(final_results_df)\n",
    "\n",
    "    print(f\"Đang lưu tất cả kết quả vào bảng {TARGET_TABLE}...\")\n",
    "    \n",
    "    try:\n",
    "        results_spark_df = spark.createDataFrame(final_results_df)\n",
    "        (results_spark_df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\") \n",
    "            .saveAsTable(TARGET_TABLE)\n",
    "        )\n",
    "        print(\"--- LƯU KẾT QUẢ VÀO DELTA LAKE THÀNH CÔNG ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"LỖI khi lưu vào Delta Lake: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0541020-58da-44aa-9eca-6c56bc128816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4,
    "widgetLayout": []
   },
   "notebookName": "predict_temp_lstm",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
